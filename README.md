Our project, named “Dream Time Machine” is a audio-enhanced visual experience, where the computer’s camera live feed is dynamically morphed and changed by Google’s Deep Dream technology into trippy dream-like visuals that respond to music inputted by the user. Our minimum viable product is a program with Deep Dream editing the camera’s live feed with music playing concurrently. The next step would be having aspects of the music (e.g. BPM, tones, volume, etc.) change aspects of the visuals, such as color, rate of visualization morphism, etc. Our dream goal is to have some way for the user to interact with the program  such that it changes what’s being displayed on the screen.
